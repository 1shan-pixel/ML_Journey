{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1229, -0.2057]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.input_layer = nn.Linear(4,6)\n",
    "        self.second_layer = nn.Linear(6,6)\n",
    "        self.final_layer = nn.Linear(6,2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        #layer.forward(X) <-> layer(X)\n",
    "        return self.final_layer(self.second_layer(self.input_layer(X)))\n",
    "\n",
    "model = Model()\n",
    "print(model.forward(torch.randn(1,4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Neural Networks through PyTorch | Given above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now playing around with basic functions in PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "from torchtyping import TensorType\n",
    "\n",
    "# Helpful functions:\n",
    "# https://pytorch.org/docs/stable/generated/torch.reshape.html\n",
    "# https://pytorch.org/docs/stable/generated/torch.mean.html\n",
    "# https://pytorch.org/docs/stable/generated/torch.cat.html\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html\n",
    "\n",
    "# Round your answers to 4 decimal places using torch.round(input_tensor, decimals = 4)\n",
    "class Solution:\n",
    "    def reshape(self, to_reshape: TensorType[float]) -> TensorType[float]:\n",
    "        return torch.round(torch.reshape(to_reshape, (-1,2)), decimals = 4)\n",
    "        # torch.reshape() will be useful - check out the documentation\n",
    "    \n",
    "\n",
    "    def average(self, to_avg: TensorType[float]) -> TensorType[float]:\n",
    "        # torch.mean() will be useful - check out the documentation\n",
    "        return torch.round(torch.mean(to_avg,dim = 0),decimals = 4)\n",
    "\n",
    "    def concatenate(self, cat_one: TensorType[float], cat_two: TensorType[float]) -> TensorType[float]:\n",
    "        # torch.cat() will be useful - check out the documentation\n",
    "        cat =  torch.cat((cat_one, cat_two), dim=1)\n",
    "        return torch.round(cat, decimals = 4) \n",
    "\n",
    "    def get_loss(self, prediction: TensorType[float], target: TensorType[float]) -> TensorType[float]:\n",
    "        return torch.round(torch.nn.functional.mse_loss(prediction, target),decimals = 4)\n",
    "        \n",
    "        # torch.nn.functional.mse_loss() will be useful - check out the documentation\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DropOut \n",
    "\n",
    "Fixiing the problem of overfitting in the cases of neural networks. \n",
    "\n",
    "Having many many layers can cause the problem of overfitting. \n",
    "\n",
    "By making the model \"dumber\" , testing accuracy has been seen to be improved. Don't know why. \n",
    "\n",
    "## torch_manualseed(0) \n",
    "\n",
    "This helps to initalize the weights randomly, the weights should be the same if we want the results to be consistent every time we run the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing a simple neural network using the supposed MNIST dataset, data has not been provided here, only tepmalate code for me to review later on. \n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtyping import TensorType\n",
    "\n",
    "class DigitRecognition(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        self.first_layer = nn.Linear(784,512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.second_layer = nn.Linear(512,10)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # Define the architecture here\n",
    "    \n",
    "    def forward(self, images: TensorType[float]) -> TensorType[float]:\n",
    "        torch.manual_seed(0)\n",
    "        x = self.first_layer(images)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.second_layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "        # even better than above code is : \n",
    "        # return self.sigmoid(self.second_layer(self.dropout(self.relu(self.first_layer(images)))))\n",
    "    \n",
    "        \n",
    "        # Return the model's prediction to 4 decimal places\n",
    "\n",
    "recognizer = DigitRecognition()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer =    torch.optim.Adam(recognizer.parameters())\n",
    "\n",
    "epochs = 10 \n",
    "\n",
    "train_dataloader = None # none for now as I have not loaded the data currently . \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in train_dataloader:\n",
    "        images = images.view(images.shape[0], 784) #flattens the 28x28 image into a 784 vector, the rest 2d tensor is the batch size ofcourse. \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad() \n",
    "        outputs = recognizer(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step() # updates the weights and biases of the neural networks \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer | Natural Language Processing \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtyping import TensorType\n",
    "\n",
    "# torch.tensor(python_list) returns a Python list as a tensor\n",
    "class Solution:\n",
    "    def get_dataset(self, positive: List[str], negative: List[str]) -> TensorType[float]:\n",
    "        # first convert the positive and negative list into a set to get unique words\n",
    "        unique_set = set()\n",
    "        for sentence in positive :\n",
    "            for words in sentence.split():\n",
    "                unique_set.add(words)\n",
    "        for sentence in negative: \n",
    "            for words in sentence.split():\n",
    "                unique_set.add(words)\n",
    "        \n",
    "        #sort the words \n",
    "        sorted_unique_set  = sorted(list(unique_set))\n",
    "        word_to_int = {}\n",
    "        # then create integer mapping for each of those words \n",
    "        for i in range(len(sorted_unique_set)):\n",
    "            word_to_int[sorted_unique_set[i]] = i+1 #this can be simplified using the enumerate function \n",
    "\n",
    "        # now we create a tensor , \n",
    "        tensors = []\n",
    "\n",
    "        for sentence in positive:\n",
    "            tensor_list = []\n",
    "            for words in sentence.split():\n",
    "                tensor_list.append(word_to_int[words])\n",
    "            tensors.append(torch.tensor(tensor_list))    \n",
    "\n",
    "        for sentence in negative:\n",
    "            tensor_list = []\n",
    "            for words in sentence.split():\n",
    "                tensor_list.append(word_to_int[words])\n",
    "            tensors.append(torch.tensor(tensor_list))   \n",
    "        \n",
    "        return(torch.nn.utils.rnn.pad_sequence(tensors, batch_first = True))\n",
    "\n",
    "\n",
    "''' \n",
    "Input:\n",
    "positive = [\"Dogecoin to the moon\"]\n",
    "negative = [\"I will short Tesla today\"]\n",
    "\n",
    "Output: [\n",
    "  [1.0, 7.0, 6.0, 4.0, 0.0],\n",
    "  [2.0, 9.0, 5.0, 3.0, 8.0]\n",
    "]\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
