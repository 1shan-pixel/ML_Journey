{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 : prepare data set \n",
    "#Step 2 : embed the data set \n",
    "#Step 3 : instantiate the model and neural network configurations \n",
    "##Step 4 : train the model \n",
    "#Step 5 : evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2, 10,  3,  6,  0],\n",
      "        [ 1,  9, 12,  8,  4],\n",
      "        [ 0,  5, 11,  7,  0]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "strings = [\n",
    "    \"The quick brown fox\",\n",
    "    \"Jumps over the lazy dog\",\n",
    "    \"Another example sentence here\" \n",
    "]\n",
    "\n",
    "def mapping(strings):\n",
    "    unique_words = set()\n",
    "    for sentence in strings: \n",
    "        for word in sentence.split():\n",
    "            unique_words.add(word)\n",
    "    \n",
    "    sorted_unique_words = sorted(list(unique_words))\n",
    "    word_to_index = {}\n",
    "    for i , word in enumerate(sorted_unique_words):\n",
    "        word_to_index[word] = i\n",
    "    # this prints {'Another': 0, 'Jumps': 1, 'The': 2, 'brown': 3, 'dog': 4, 'example': 5, 'fox': 6, 'here': 7, 'lazy': 8, 'over': 9, 'quick': 10, 'sentence': 11, 'the': 12}\n",
    "    \n",
    "    returnee_tensor = []\n",
    "\n",
    "    for sentence in strings: \n",
    "        current_tensor = []\n",
    "        for word in sentence.split():\n",
    "            current_tensor.append(word_to_index[word])\n",
    "        returnee_tensor.append(torch.tensor(current_tensor))\n",
    "    \n",
    "    return nn.utils.rnn.pad_sequence(returnee_tensor,batch_first=True)\n",
    "\n",
    "''' \n",
    "prints this : \n",
    "tensor([[ 2, 10,  3,  6,  0],\n",
    "        [ 1,  9, 12,  8,  4],\n",
    "        [ 0,  5, 11,  7,  0]])\n",
    "'''\n",
    "\n",
    "# The embedding layer effectively acts as a lookup table. \n",
    "#one hot enconding fails when the vocabulary size is too large, also fails to capture the relationship between words\n",
    "class Sentiment(nn.Module):\n",
    "    def __init__(self, embedding, ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(12, 16)\n",
    "        self.linear = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "  \n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding_layer(x)  # this is the embedding layer , this coverts the input into a tensor of shape (sequence_length, embedding_size)\n",
    "        averaged = torch.mean(embeddings, axis = 1) # this is the average pooling layer , this averages the embeddings across the sequence length, so across the rows\n",
    "        projected = self.linear_layer(averaged) # this is the linear layer , this projects the embeddings into a single value\n",
    "        return self.sigmoid(projected)\n",
    "\n",
    "# anyways some sort of training loop and and model validation needs to be done, I shall do this later \n",
    "# J hos bujiyo yo sentiment analysis ni\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
